import torch

def generate_answer(query, context, manager):
    prompt = f"""[INST] Vá»›i tÆ° cÃ¡ch lÃ  má»™t trá»£ lÃ½ phÃ¡p lÃ½ chuyÃªn nghiá»‡p, dá»±a trÃªn vÄƒn báº£n luáº­t sau:

{context}

HÃ£y tráº£ lá»i cÃ¢u há»i sau má»™t cÃ¡ch chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§:
{query}

YÃªu cáº§u:
1. Tráº£ lá»i ngáº¯n gá»n, sÃºc tÃ­ch
2. TrÃ­ch dáº«n Ä‘iá»u khoáº£n cá»¥ thá»ƒ
3. Äáº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c cao
4. Äá»‹nh dáº¡ng markdown [/INST]"""

    with torch.cuda.amp.autocast():
        with torch.no_grad():
            inputs = manager.vinallama_tokenizer.encode(
                prompt, 
                return_tensors="pt"
            ).to(manager.device)

            outputs = manager.vinallama_model.generate(
                inputs,
                max_length=768,
                num_beams=5,
                temperature=0.7,
                top_p=0.9,
                repetition_penalty=1.2,
                early_stopping=True,
                pad_token_id=manager.vinallama_tokenizer.eos_token_id
            )

    answer = manager.vinallama_tokenizer.decode(outputs[0], skip_special_tokens=True)

    return f"""
### ğŸ“ CÃ¢u há»i
{query}

### âœ… Tráº£ lá»i
{answer}

### ğŸ“š Nguá»“n tham chiáº¿u
```text
{context[:200]}...
```"""
